
---

ayout: AI  
title: "퍼셉트론, 선택의 순간들"  
comments: true  
categories:[AI]  
tags:[AI]

---

# MS AI School 1차 프로젝트 회고

MS AI School에서 공부한 지도 어느덧 두 달.
설렘과 긴장이 뒤섞인 채 첫 프로젝트를 마주했다.

프로젝트를 시작하기 약 2주 전,  
미팅홀에서 우연히 만났던 분이 먼저 연락을 주셨다.  
자율적으로 팀을 꾸려야 하는 과정이 막막하기만 했는데,  
그 연락 하나가 불안했던 마음을 단숨에 덜어주었다.

> _혼자 고민하며 머뭇거리고 있을 때, 먼저 건네준 손길이 참 고마웠다.

그렇게 나를 포함하여 7명이 한 팀으로 모였다.  
모두가 열정적이고 성격 또한 다정한 사람들이었다.  
서로를 알아갈수록, 살아온 환경과 나이는 달라도  
각자에게서 배울 것이 참 많다는 걸 느꼈다.

>_이토록 좋은 이들과 함께 성장할 수 있다는 것은, 삶이 주는 작은 선물 같았다._

균형을 맞추기 위해 팀을 다시 편성하게 되었다.  
다행히도 기존에 짜여진 팀들의 의견을 최대한 반영하여,  
우리 팀은 4명과 3명으로 나누어졌다.

비록 처음부터 끝까지 함께할 순 없었지만,  
그렇다고 해서 이 인연이 쉽게 흐려질 리는 없었다.

우리는 다시 만날 날을 기약하며, 채팅방 이름을 **‘견우와 직녀팀’**으로 바꿨다.  
지금도 여전히 톡방은 활발하다.  
매일같이 즐겁게 대화를 나누고, 서로의 하루를 응원하며,  
**‘강철부대’(운동·다이어트 인증방)**도 함께 운영하며 좋은 인연을 이어가고 있다.

> _그런 사람들을 만났다는 것만으로도, 내게 이 프로젝트는 이미 값진 시작이었다._

---

## 프로젝트 아이디어 선정 기준

새롭게 구성된 팀은 기존 4명과 새로운 3명이 함께하게 되었다. 
각자 다양한 아이디어를 고민했지만, 
나는 지난 두 달간 배운 것을 토대로 다음과 같은 기준을 세웠다.


- 최신 뉴스나 사회적 이슈로 주목받는 주제 (우선 순위)
- 컴퓨터 비전(이미지 처리, 수치화 처리) 기술을 활용할 것
- 충분히 공개된 데이터셋이 존재해야 함.
- 이미 상용화된 서비스는 배제하고, 경쟁이 적어야 함.
- 아직 명확한 해결책이 나오지 않은 문제를 다룰 것.

이 기준을 바탕으로 취업 포트폴리오로도 손색없고,  
IT와 깊이 연결되는 아이디어를 찾고자 했지만,  
모든 조건을 만족하는 주제를 찾기란 쉽지 않았다.

결국, 여러 고민 끝에 팀장님의 아이디어를 채택하기로 했고,  
이번 프로젝트의 주제는 **‘반려견/반려묘 피부질환 AI 판별 서비스’**로 정해졌다.

**프로젝트 기간은 단 2주. ** 
 나는 모델 파트를 담당하게 되었다.

---

## 모델 파트의 업무 계획

내가 담당한 모델 파트의 업무는 다음과 같은 계획으로 진행했다.

|단계|업무 내용|
|---|---|
|1|데이터셋 구조와 특성 이해|
|2|각 피부 질환별 데이터 분류|
|3|데이터 전처리 과정 진행|
|4|Azure Custom Vision(CV)을 통한 모델 학습|
|5|파이토치의 ResNet 모델을 활용한 학습 진행|
|6|CV와 파이토치 모델 성능 비교 분석|
    
---
## 데이터셋 분석

데이터셋은 382GB에 달했고, 다운로드와 압축을 푸는 데만 하루가 걸렸다. 
방대한 데이터 양 앞에서 처음에는 막막함이 몰려왔다.

**데이터셋의 구조**

📁 반려동물_피부질환_데이터셋
 ├── 📁 Dog (반려견)
 │    ├── 📁 유증상 / 무증상 
 │    │    ├── 📁 A1_구진_플라크
 │    │    ├── 📁 A2_비듬_각질_상피성잔고리
 │    │    ├── 📁 A3_태선화_과다색소침착
 │    │    ├── 📁 A4_농포_여드름
 │    │    ├── 📁 A5_미란_궤양
 │    │    ├── 📁 A6_결절_종괴
 ├── 📁 Cat (반려묘)
 │    ├── 📁 유증상 / 무증상 
 │    │    ├── 📁 A2_비듬_각질_상피성잔고리
 │    │    ├── 📁 A4_농포_여드름
 │    │    ├── 📁 A6_결절_종괴

---

## 데이터 분류와 초기 테스트 과정

모델 학습을 시작하기 전, 각 태그마다 **50장씩**을 선택해 테스트를 진행했다.  
하지만 결과는 기대에 못 미쳤다.

데이터의 양이 부족한 탓이라 생각하고,  
각 태그당 **3,000장씩**으로 늘려 다시 테스트했지만,  
여전히 성능은 개선되지 않았다.

결국, **데이터의 양보다는 품질이 더 중요한 요소**라는 결론을 내렸다.  
그래서 각 태그에서 **가장 선명하고 특징이 잘 드러나는 이미지 100장씩**을 선별하고,  
원본과 전처리된 데이터를 일정 비율로 섞어 다시 실험을 진행했다.

---

## Azure Custom Vision 프로젝트 설정 과정

Azure Custom Vision에서 새로운 프로젝트를 만들 때 다음과 같이 설정했다.

- **Project Types** : Classification (분류)
- **Classification Types** : Multilabel
- 
![[Pasted image 20250309225352.png]]

처음에는 Azure Custom Vision에 대한 정보가 부족했기에,  
직접 여러 가지 설정을 시도하며 학습해보기로 했다.  
특히 Training과 Validation 비율을 **8:2**로 조정하고 싶었기 때문에,  
여러 개의 라벨을 동시에 예측할 수 있는 **Multilabel** 방식을 선택했다.

하지만 학습을 진행하면서 예상치 못한 문제가 발생했다.  
Multilabel 방식에서는 원하는 대로 비율을 조정하기 어려웠고,  
데이터 불균형 문제가 심화되면서 모델의 성능이 제대로 나오지 않았다.

결국, **Multiclass (싱글태그) 방식으로 변경하여 모델을 다시 학습**했다.  
이후 실험을 통해 데이터 불균형 문제를 해결하는 것이 더 중요하다는 걸 깨닫게 되었다.

---

## 강아지 피부 질병 모델 학습의 시행착오

강아지 피부 질병 모델을 구축하는 과정에서,  
우리는 총 **6가지(A1~A6)**의 질병을 분류하는 모델을 학습시켜야 했다.

처음에는 원본 데이터만으로 학습을 진행했지만,  
예상했던 것보다 성능이 좋지 않았다.  
모델이 학습을 제대로 하지 못하는 원인을 분석해보니,  
동물의 피부 특성상 촬영된 부위와 털의 상태가 제각각이어서  
질병의 특징을 효과적으로 학습하는 데 어려움이 있다는 점을 발견했다.

또한, 한 번에 **A1~A6** 모든 질병을 학습시키자 모델의 성능이 더욱 저하되는 문제가 발생했다.  
그래서 학습 방식을 변경하여, 3단계(Iteration)로 나누어 진행하기로 했다.

|Iteration|학습할 질병|
|---|---|
|**Iteration 1**|A1 + A2|
|**Iteration 2**|A5 + A6|
|**Iteration 3**|A3 + A4|

다양한 조합을 테스트해본 결과, 위와 같은 학습 순서가 가장 높은 성능을 보였다.  
이를 통해 **질병의 유형과 학습 순서 또한 모델 성능에 영향을 미친다**는 점을 알게 되었다.

---

## 전처리 과정 및 성능 테스트

### **전처리 방식 선정 과정**

모델의 성능을 향상시키기 위해,  
단순히 데이터를 증가시키는 것이 아니라 **데이터 품질을 개선하는 방법**이 필요하다고 판단했다.  
그래서 최적의 전처리 과정을 찾기 위해 여러 테스트를 진행했다.

처음 고려했던 전처리 방법은 다음과 같았다.

- **배경제거**
- **정규화(Normalization)**
- **색변환(Color Augmentation)**
- **히스토그램 평활화(Histogram Equalization)**

하지만 배경제거 과정에서 오히려 모델이 혼란을 겪는 문제가 발생했다.  
따라서 배경제거는 제외하고, 대신 **채도(Saturation) 조정**을 추가하여 테스트를 진행했다.

또한, 단일 전처리 방식뿐만 아니라 **복합적인 조합(믹스 전처리)**도 실험했다.

- **정규화 + 샤프닝 + 채도 조정**
- **정규화 + 히스토그램 평활화**
- **정규화 + 색변환**

이렇게 다양한 방식으로 전처리를 적용한 후,  
각각의 전처리 방식이 모델 성능에 미치는 영향을 비교 분석했다.

---
### **전처리 방식별 성능 비교 및 최적의 조합 찾기**

모델의 성능을 평가하기 위해 다음 지표를 사용했다.

- **Precision (정밀도)** : 모델이 "정답"이라고 예측한 것 중 실제 정답인 비율
- **Recall (재현율)** : 실제 정답 중에서 모델이 올바르게 맞힌 비율
- **AP (평균 정밀도, Average Precision)** : Precision-Recall 곡선을 기반으로 한 종합 성능 지표

#### **Iteration 1 (A1 + A2) 전처리 방식별 성능 비교**

|전처리 방법|Precision|Recall|AP|
|---|---|---|---|
|색변환|72.5|72.5|81.0|
|정규화|75.0|75.0|85.5|
|히스토그램 평활화|80.0|80.0|84.5|
|**정규화 + 히스토그램 평활화**|**80.0**|**80.0**|**90.6**|

#### **Iteration 2 (A5 + A6) 전처리 방식별 성능 비교**

|전처리 방법|Precision|Recall|AP|
|---|---|---|---|
|색변환|90.0|90.0|96.6|
|히스토그램 평활화|85.0|85.0|92.2|
|정규화 + 히스토그램|75.5|75.5|88.4|

#### **Iteration 3 (A3 + A4) 전처리 방식별 성능 비교**

|전처리 방법|Precision|Recall|AP|
|---|---|---|---|
|정규화|67.5|67.5|69.7|
|색변환|75.0|75.0|73.7|
|히스토그램 평활화|100.0|50.0|100.0|
|**정규화 + 샤프닝 + 채도 조정**|**87.5**|**87.5**|**92.0**|

#### **최적의 전처리 데이터를 적용한 최종 실험 결과**

|Iteration|전처리 방식|Precision|Recall|AP|
|---|---|---|---|---|
|Iteration 1 (A1 + A2)|정규화 + 히스토그램|80.0|80.0|90.6|
|Iteration 2 (A5 + A6)|색변환|84.8|83.8|92.0|
|Iteration 3 (A3 + A4)|정규화 + 샤프닝 + 채도|82.5|82.5|91.6|

최적의 전처리 조합을 적용하여 다시 모델을 학습한 결과,  
Precision, Recall, AP 값이 전반적으로 개선되었다.

>그러나, 예상치 못한 새로운 벽이 눈앞에 나타났다

---  
## **Quick Test에서 발견한 문제점**

최적의 전처리를 적용한 후, 모델의 성능은 눈에 띄게 향상되었다.  
하지만 Quick Test(실제 테스트 데이터 투입)에서는 예상과 다른 결과가 나왔다.

**테스트 이미지에 따라 모델이 올바르게 분류하지 못하는 현상이 발생했다.**  
이를 분석해보니, **원본 데이터의 중요성**을 간과했던 것이 문제였다.

단순히 전처리 데이터를 추가하는 것이 아니라,  
**원본 데이터를 적절히 혼합해 학습시키는 것이 필수적**이라는 점을 깨달았다.

---

## **Negative 태그와 데이터 불균형 문제**

모델 성능 개선을 위해 다양한 전처리를 적용했지만,  
Quick Test 결과는 여전히 기대에 미치지 못했다.  
이유를 분석하던 중, **Negative 태그**의 개념을 처음 접하게 되었다.

### **Negative 태그란?**

- 모델이 특정 클래스로 **분류하면 안 되는 이미지**를 학습하는 기능
- 오탐(False Positive)을 줄여, **잘못된 분류를 최소화하는 역할**

이 개념을 이해하고 나서야,  
왜 데이터셋에 **무증상 폴더**가 포함되어 있었는지 비로소 깨달았다.

즉, **유증상 데이터(A1~A6)는 Positive, 무증상 데이터는 Negative**로 처리해야  
모델이 더 정확한 판단을 내릴 수 있었다.

### **데이터 불균형 문제와 해결 과정**

처음 학습할 때,  
Positive(유증상) 데이터만 집중적으로 학습시키면서  
Negative(무증상) 데이터의 중요성을 간과하고 있었다.

그러나 **Negative 데이터가 적을 경우, 모델이 과적합되거나 오탐이 증가하는 문제**가 발생한다.  
이를 해결하기 위해, Positive와 Negative 데이터를 **1:1 비율로 맞추는 전략**을 도입했다.

> **데이터의 양이 많다고 해서 성능이 무조건 좋아지는 것이 아니다.**  
> **균형 잡힌 데이터 분포가 모델의 신뢰도를 결정한다.**

이제야 데이터 불균형이 모델 성능에 미치는 영향을 체감할 수 있었다.  
하지만, 또 다른 문제가 남아 있었다.  

---

## **False Positive & False Negative 문제와 데이터 검토**

전처리 방식과 데이터 비율을 조정하며 학습을 진행했지만,  
**False Positive(FP, 오탐)와 False Negative(FN, 미탐) 문제가 여전히 해결되지 않았다.**

- **False Positive (오탐)** : 무증상(정상) 이미지를 특정 질환으로 잘못 분류
- **False Negative (미탐)** : 실제 질환 이미지를 무증상으로 잘못 분류

이 문제는 모델의 신뢰도를 떨어뜨렸고,  
결국 **데이터셋 자체를 다시 검토해야 한다는 결론**에 도달했다.
이제, 남은 건 마지막 한 번의 시도뿐이었다.

---

## **임계값(Threshold) 조정과 한계**

이 문제를 해결하기 위해,  
Threshold(임계값)를 조정하여 Precision과 Recall 균형을 맞추는 실험을 진행했다.

|Threshold 값|Precision 변화|Recall 변화|
|---|---|---|
|50% → 낮춤|Precision 감소|Recall 증가|
|50% → 높임|Precision 증가|Recall 감소|

- Threshold를 낮추면 Recall이 올라가지만, Precision이 떨어짐
- Threshold를 높이면 Precision은 올라가지만, Recall이 낮아짐

Precision과 Recall 값이 모두 높다면 문제가 없겠지만,  
이 당시 모델의 성능은 **60~70% 수준**이었기 때문에  
Threshold 조정만으로는 근본적인 문제를 해결할 수 없었다.

결국, **모델을 다시 학습시키기 전에 데이터셋을 처음부터 다시 점검해야 했다.**

>_흐릿한 캔버스 위에 덧칠하기보다, 깨끗한 도화지에 다시 그리는 게 낫다.

---

## **데이터셋 재검토 : 우리가 놓쳤던 것들**

나는 다시 처음으로 돌아가, 데이터셋을 하나씩 살펴보기 시작했다.

처음에 **100장씩 임의로 분류했던 이미지들**을 다시 확인하며,  
다른 증상들과 비교해본 결과, 우리는 **데이터셋에 대한 이해가 부족했다**는 걸 깨달았다.

예를 들어,

- **A1 폴더**에 있는 이미지들이 모두 A1 질병을 나타내는 것이 아니었다.  
    더 심각한 상태의 데이터도 있었고, 무증상에 가까운 데이터도 포함되어 있었다.
- **무증상 폴더**에 있어야 할 데이터가 일부 질병 폴더에 섞여 있었다.

결과적으로, **처음부터 데이터 분류를 제대로 하지 못한 상태에서 학습을 진행했기 때문에**  
모델이 혼란을 겪을 수밖에 없었다.

이제 확신할 수 있었다.  
**모델의 성능이 낮았던 이유는 데이터 자체가 뒤죽박죽이었기 때문이었다.**  
처음부터 올바르게 분류하고 정제했다면,  
더 좋은 성능을 기대할 수 있었을 것이다.

---

## **정확한 데이터 분류와 균형 조정**

결국, **처음부터 데이터를 다시 정리해야 한다는 결론**에 도달했다.  
팀원들에게 데이터를 새롭게 분류하자고 제안했고,  
단순히 양을 늘리는 것이 아니라 **각 질병에 맞는 데이터를 더욱 명확하게 분류하는 것**에 초점을 맞췄다.

이를 위해 **분류해야 할 데이터의 양을 두 배(100장 → 200장)로 늘려서 진행하였다.

새로운 데이터 분류 작업에 하루라는 시간이 추가로 소요되었지만,  
이미 학습 과정에서 충분한 경험과 데이터를 쌓아왔기에  
모델을 다시 완성하는 과정은 더 이상 어렵지 않았다.

그리고, 그 결과는 확실했다.

기존에는 **Negative(무증상) 데이터에 원본 데이터만 포함**했지만,  
이번에는 **전처리 데이터를 함께 포함하는 방식**으로 변경했다.  
무엇보다도, **각 질병의 특성이 확실하게 드러나는 데이터를 선별하여 분류를 정교하게 다듬었다.**  
**단순한 균형 조정이 아니라, 데이터 자체를 질적으로 향상시키는 과정**이었다.

그렇게, **더욱 정확하고 정제된 데이터셋을 기반으로 모델을 다시 학습했다.**

---

 ## Iteration 별 학습 결과 및 데이터 구성

각 Iteration마다 **질병별 데이터 구성과 전처리 조합을 최적화**하며 학습을 진행했다.  
이 과정에서 **Negative(무증상)와 Positive(유증상)의 균형을 맞추고**,  
각 질병별로 **가장 효과적인 전처리 기법을 적용하는 것**이 성능 향상에 중요한 역할을 했다.

### **🔹 모델 성능 비교 (Iteration별 Precision / Recall / AP)**

|Iteration|질병군|Precision|Recall|AP|
|---|---|---|---|---|
|**Iteration 1**|A1 + A2|**90.3**|**97.9**|**94.0**|
|**Iteration 2**|A3 + A4|**84.9**|**80.1**|**89.8**|
|**Iteration 3**|A5 + A6|**82.2**|**71.5**|**84.7**|****

### **Iteration 1 (A1 + A2) 데이터 구성**

📌 **Negative (무증상) 데이터** _(총 550장)_

- 원본 : **250장**
- 밝기 20% 조정 : **100장**
- 정규화 : **100장**
- 좌우반전 : **100장**

📌 **Positive (A1 / A2) 데이터** _(총 550장)_

- 원본 : **200장**
- 최적의 전처리(정규화 + 히스토그램) : **100장**
- 정규화 : **100장**
- 색변환 : **80장**
- 채도 조정 : **70장**

> **A1(구진,플라크), A2(비듬,각질,상피성잔고리)**의 데이터는 
> 털의 색상과 피부 상태의 차이가 미묘하여,  
> 정규화와 히스토그램 평활화가 가장 효과적이었다.**  
> 색변환과 채도 조정을 추가하여 피부의 미세한 변화를 더 뚜렷하게 학습하도록 했다.

---

### **🔹 Iteration 2 (A3 + A4) 데이터 구성**

📌 **Positive (A3 / A4) 데이터** _(총 550장)_

- 원본 : **200장**
- 최적의 전처리(채도 조정) : **100장**
- 정규화 : **100장**
- 색변환 : **75장**
- 정규화 + 히스토그램 : **75장**

> **A3(태선화, 과다색소침착), A4(농포, 여드름) 데이터는 피부의 색 대비가 강한 편이었다.**  
> 따라서 채도 조정을 활용하여 모델이 피부 조직의 변화를 명확히 인식하도록 했다.

---

### **🔹 Iteration 3 (A5 + A6) 데이터 구성**

📌 **Positive (A5 / A6) 데이터** _(총 550장)_

- 원본 : **200장**
- 최적의 전처리(색변환) : **100장**
- 정규화 : **100장**
- 채도 조정 : **75장**
- 정규화 + 히스토그램 : **75장**

> **A5(미란, 궤양), A6(결절, 종괴) 질병은 피부 조직이 심하게 손상된 경우가 많았다.**  
> 색변환을 통해 병변의 형태적 차이를 강조하는 것이 학습에 효과적이었다.

---

### **🔹 학습 결과 정리**

- 각 Iteration에서 **Negative(무증상)와 Positive(유증상)의 데이터 비율을 1:1(550:550)로 맞추어 데이터 불균형을 방지**했다.
- 각 질병 유형에 **가장 적절한 전처리 기법을 적용**하여 모델의 질병 구분 능력을 극대화했다.
- **Iteration을 나누어 학습하면서, 각 질병별 특성이 더 뚜렷하게 반영된 데이터셋을 구축**할 수 있었다.

> **결국, 단순히 데이터를 늘리는 것이 아니라,  
> 모델이 가장 효과적으로 학습할 수 있도록 데이터를 선별하고 가공하는 것이 성능 향상의 핵심이었다.**


---

## **최종 정리: 데이터 정제와 학습 과정의 핵심 정리**

이번 프로젝트의 핵심은 단순히 모델을 학습시키는 것이 아니었다.  
**질 높은 데이터를 선별하고, 정교한 전처리를 적용하며, 균형 잡힌 학습을 설계하는 과정 자체가 모델의 성능을 결정했다.**  
이제, 우리가 진행한 전체 과정과 결과를 최종적으로 정리해보려 한다.

---

### ** 1. 데이터 선별: 단순한 양 증가가 아닌, 정확한 데이터 정리**

모델의 성능을 높이기 위해 **각 질병별 특징이 확실한 데이터를 선별하는 것**이 가장 중요한 단계였다.  
처음에는 100장씩 샘플링했지만, 질병의 경중에 따라 분류가 애매한 데이터들이 많았다.  
따라서 **각 질병에 대해 200장씩 선별하여 데이터의 질을 높이는 작업을 진행했다.**

🔸 **데이터 선별 기준**  
🔸**질병의 특성이 뚜렷한 이미지 우선 선택**  
🔸 **배경 요소(털, 조명 등)에 영향을 덜 받는 이미지 선정**  
🔸 **질병별 유사도를 고려하여 정확한 분류 진행**

---

### **2️. 데이터 전처리: 최적의 조합을 찾다**

처음에는 **배경제거, 정규화, 색변환, 히스토그램 평활화** 등의 전처리 기법을 사용했지만,  
배경제거의 효과가 크지 않다고 판단하여 제외했다.  
대신, **정규화 + 히스토그램 평활화, 정규화 + 샤프닝 + 채도 조정 등**  
데이터 유형에 따라 **최적의 전처리 조합을 적용**했다.

✔ **최종 전처리 방식**  
 🔸 정규화 (Normalization)  
 🔸 히스토그램 평활화 (Histogram Equalization)  
 🔸 색변환 (Color Transformation)  
 🔸채도 조정 (Saturation Adjustment)

각 데이터 유형별로 가장 적절한 전처리를 적용하여 **모델이 핵심적인 특징을 학습할 수 있도록 유도했다.**

---

### **3️. 데이터 균형 조정: Negative와 Positive의 1:1 비율**

처음에는 **무증상(Negative) 데이터가 모델 학습에 미치는 영향**을 고려하지 않았다.  
그러나 Negative 태그를 학습에 활용하면 오탐(False Positive)을 줄일 수 있다는 점을 깨달았고,  
결국 **Negative(무증상)과 Positive(유증상)의 데이터를 1:1 비율로 맞추는 것이 중요하다는 결론에 도달했다.**

|**데이터 유형**|**구성 내용**|**총 이미지 수**|
|---|---|---|
|**Negative (A0 - 무증상)**|원본 250장 / 밝기 조정 100장 / 정규화 100장 / 좌우반전 100장|**550장**|
|**Positive (A1~A6)**|원본 200장 / 최적 전처리 데이터 140장 / 추가 전처리 70장 + 70장 + 70장|**550장 (각 질병당)**|

**✔ Negative : Positive 비율을 1:1 (550:550)로 맞추어 데이터 불균형을 해결**  
**✔ 각 질병별 데이터 정밀 분류를 통해 모델 학습의 신뢰도를 높임**

---

### **4️. Custom Vision 학습 전략**

모델의 학습은 단순한 반복이 아니라, **데이터 특성을 고려한 단계적 접근이 필수적**이었다.  
이를 위해 **3단계 Iteration을 구성**하고, 각 Iteration마다 모델의 성능을 평가했다.

#### **Iteration 구성 및 학습 순서**

|**Iteration 단계**|**질병 그룹**|**비고**|
|---|---|---|
|**Iteration 1**|Negative + A1 + A2|**피부 변화가 미묘한 초기 증상** (정규화, 히스토그램 중심)|
|**Iteration 2**|A5 + A6|**가장 심각한 증상 그룹** (색변환, 채도 조정 효과적)|
|**Iteration 3**|A3 + A4|**중간 단계 증상** (정규화 + 히스토그램 조합이 유효)|

✔ **Iteration 진행 시 Quick Test를 수행하여 성능 분석**  
✔ **잘못 분류된 데이터(태깅 오류)를 찾아 삭제 후 재학습**  
✔ **반복적으로 오류가 발생하는 태그는 데이터 보완 후 유지**

---

### **5️. 최종 모델 성능 결과**

최적의 데이터 조합을 적용한 후, 모델의 성능이 어떻게 향상되었는지 살펴보았다.

|**Iteration**|**Precision**|**Recall**|**AP (평균 정밀도)**|
|---|---|---|---|
|**Iteration 1 (A1 + A2)**|90.3|97.9|94.0|
|**Iteration 2 (A3 + A4)**|84.9|80.1|89.8|
|**Iteration 3 (A5 + A6)**|82.2|71.5|84.7|

✔ **Negative 데이터 활용 및 데이터 정제를 통해 Precision(정밀도) 상승**  
✔ **질병별 전처리 조합 최적화 후 AP(평균 정밀도) 대폭 향상**  
✔ **Recall(재현율)은 추가적인 데이터 보강이 필요**

---

## 최종 모델 조정: A1/A2 제거 및 성능 최적화

원하는 성능까지 끌어올리지 못한 가장 큰 원인은 **A1~A4 질병 간의 경계가 모호했다는 점**이었다.  
특히, A1/A2는 증상이 비교적 경미하고, 다른 질병과의 구분이 어려워  
모델이 정확한 예측을 하지 못하는 경우가 많았다.

무엇보다도, **주어진 시간 내에 데이터 정제와 추가적인 전처리를 충분히 진행하기 어려웠다.**  
더 많은 데이터를 검토하고 보완할 시간이 있었다면,  
A1/A2를 포함한 모델 성능 개선도 가능했을지 모른다.

그러나 **현실적인 제한 속에서 최선의 선택을 해야 했다.**  
이에 따라 강아지 모델에서 A1/A2를 제거하고, **A3~A6로 구성하는 방향으로 조정**했다.  
결과적으로 모델의 혼란을 줄이고, 예측 성능을 높이는 선택이 되었다.

### **최종 강아지 모델 성능 분석**

최종적으로 **Iteration 5**에서 모델 성능을 평가한 결과,  
Precision, Recall, AP(평균 정밀도) 값이 모두 안정적인 수준으로 도달했다.

![[스크린샷 2025-02-25 092840.png]]
#### **🔍 최종적인 조정 사항**

- **Recall 값이 70%대에 머물렀던 A6, A3의 성능을 개선하기 위해 임계값(Threshold)을 조정하여 80% 이상으로 맞췄다.**
- **False Positive (FP, 오탐) 및 False Negative (FN, 미탐) 케이스를 최소화하는 방식으로 태그를 수정하였다.**
- **Negative(무증상) 데이터의 경우 가장 높은 AP(97.5%)를 기록하며, 모델이 질환이 없는 경우를 가장 잘 판별하는 것으로 확인되었다.**

> **🔥 결론적으로, A1/A2를 제외하고 A3~A6으로 조정한 것이 성능 개선에 효과적이었다.**  
> **Threshold 조정을 통해 Recall이 보완되었으며, Precision과 AP 역시 우수한 결과를 기록하였다.**


### **최종 고양이 모델 성능 분석**

Iteration 3을 기준으로 모델 성능을 평가한 결과,  
Precision, Recall, AP(평균 정밀도) 값이 안정적인 수준에 도달했다.

![[Pasted image 20250311002931.png]]
#### **🔍 최종적인 조정 사항**

- **Threshold(임계값)는 조정하지 않고 기본 50%를 유지했다.**
- **False Positive (FP, 오탐) 및 False Negative (FN, 미탐)는 1회씩만 걸러냈다.**
- **Negative(무증상) 데이터가 가장 높은 AP(96.8%)를 기록하며, 모델이 질환이 없는 경우를 비교적 정확하게 판별하는 것으로 확인되었다.**

> **🔥 결론적으로, A4, A2, A6으로 구성한 모델은 강아지 모델보다 안정적인 성능을 기록했다.**  
> **Threshold를 조정하지 않고도 Recall 값이 80% 이상 유지되었으며, Precision과 AP 역시 만족스러운 수준을 보였다.**

---

### **1차 프로젝트 회고**

이번 프로젝트를 통해 얻은 가장 큰 교훈은,  
단순히 **데이터의 양을 늘리는 것이 아니라, 학습에 적합한 데이터를 정제하고 선별하는 것**이  
모델 성능 향상의 핵심이라는 점이었다.

모델이 아무리 뛰어나더라도,  
**데이터가 제대로 정리되지 않으면 원하는 결과를 얻을 수 없다.**  
결국, AI의 본질은 **데이터를 얼마나 깊이 이해하고 다듬느냐**의 문제라는 것을 실감했다.

특히, **무증상(Negative)과 유증상(Positive) 데이터를 균형 있게 구성하는 것**이 중요했다.  
데이터의 불균형을 해결하고, 질병의 특징이 명확한 데이터를 선별하는 과정이  
결과적으로 모델 성능을 결정지었다.

하지만 여전히 개선할 부분은 많았다.  
다음 프로젝트에서는 **더 철저한 사전 계획과 데이터 검토**를 통해  
완성도를 높이는 데 집중할 예정이다.

- **무증상 데이터(A0) 세분화** → Negative 데이터를 더 정교하게 구성하여 학습 향상
- **배경제거 기법 추가 실험** → 질병 부위를 더욱 명확하게 인식할 수 있도록 개선
- **모델 최적화 및 비교 실험** → Custom Vision뿐만 아니라 다른 모델들과 성능 비교 진행

**"좋은 데이터가 좋은 모델을 만든다."**  
단순한 알고리즘 최적화가 아닌, **데이터의 질을 높이는 것**이 가장 중요한 과제였다.  
이번 경험을 바탕으로, 다음 프로젝트에서는 더욱 체계적인 접근과 세밀한 데이터 정리를 통해  
완성도를 높이고자 한다.

> **실패에서 배운 것은 단순한 기술이 아니라, 더 깊은 통찰이었다.**  
> 모델이 아니라 **데이터를 먼저 바라봐야 한다는 것.**  
> 그리고 이 모든 과정을 함께한 팀원들과의 시간이 무엇보다도 값진 배움이었다.

---
## 프로젝트를 돌아보며

처음에는 그저 AI 모델을 구축하는 기술적 작업이라 생각했다. 하지만 이번 프로젝트를 통해 데이터에 대한 깊은 이해와 섬세한 분류 작업이 얼마나 중요한지 온전히 깨달았다. 결국, 모델은 데이터를 그대로 닮아간다. 데이터에 담긴 본질과 가치를 제대로 들여다보고 이해하는 것이 좋은 모델을 만드는 가장 중요한 과정이라는 것을 뒤늦게 깨달았다.

>돌아보면 많은 시행착오와 순간의 좌절도 있었지만,  
>그 모든 어려움이 결국 성장의 밑거름이 되었다.  
>조급함을 내려놓고, 나만의 속도로 묵묵히 걸어가야겠다고 생각했다.  
>무엇보다, 함께한 팀원들에게 깊은 감사를 전한다.  
>끝까지 고민하고 노력하며 함께했던 시간이 있었기에,  
>이 과정이 더욱 값지고 의미 있는 배움이 될 수 있었다.  
>그 소중한 순간들을 마음에 새기며, 앞으로도 함께 나아가길 바라본다.